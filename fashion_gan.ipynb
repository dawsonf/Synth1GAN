{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "\n",
    "n_cpu = 8\n",
    "latent_dim = 100\n",
    "\n",
    "img_size = 28\n",
    "channels = 1\n",
    "sample_interval = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (channels, img_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.694152] [G loss: 0.708097]\n",
      "[Epoch 0/200] [Batch 400/938] [D loss: 0.351821] [G loss: 1.369241]\n",
      "[Epoch 0/200] [Batch 800/938] [D loss: 0.213687] [G loss: 1.907099]\n",
      "[Epoch 1/200] [Batch 262/938] [D loss: 0.279028] [G loss: 1.460900]\n",
      "[Epoch 1/200] [Batch 662/938] [D loss: 0.322412] [G loss: 1.495587]\n",
      "[Epoch 2/200] [Batch 124/938] [D loss: 0.353952] [G loss: 1.227704]\n",
      "[Epoch 2/200] [Batch 524/938] [D loss: 0.323709] [G loss: 2.621522]\n",
      "[Epoch 2/200] [Batch 924/938] [D loss: 0.386572] [G loss: 2.849360]\n",
      "[Epoch 3/200] [Batch 386/938] [D loss: 0.353470] [G loss: 1.331467]\n",
      "[Epoch 3/200] [Batch 786/938] [D loss: 0.379417] [G loss: 1.114637]\n",
      "[Epoch 4/200] [Batch 248/938] [D loss: 0.247174] [G loss: 1.577502]\n",
      "[Epoch 4/200] [Batch 648/938] [D loss: 0.403204] [G loss: 1.846150]\n",
      "[Epoch 5/200] [Batch 110/938] [D loss: 0.318189] [G loss: 1.662735]\n",
      "[Epoch 5/200] [Batch 510/938] [D loss: 0.434388] [G loss: 0.899238]\n",
      "[Epoch 5/200] [Batch 910/938] [D loss: 0.338910] [G loss: 1.476771]\n",
      "[Epoch 6/200] [Batch 372/938] [D loss: 0.403200] [G loss: 1.207157]\n",
      "[Epoch 6/200] [Batch 772/938] [D loss: 0.369303] [G loss: 1.634874]\n",
      "[Epoch 7/200] [Batch 234/938] [D loss: 0.250840] [G loss: 1.626921]\n",
      "[Epoch 7/200] [Batch 634/938] [D loss: 0.345666] [G loss: 2.092738]\n",
      "[Epoch 8/200] [Batch 96/938] [D loss: 0.360962] [G loss: 1.730910]\n",
      "[Epoch 8/200] [Batch 496/938] [D loss: 0.247525] [G loss: 2.688218]\n",
      "[Epoch 8/200] [Batch 896/938] [D loss: 0.341256] [G loss: 1.345981]\n",
      "[Epoch 9/200] [Batch 358/938] [D loss: 0.306473] [G loss: 1.718369]\n",
      "[Epoch 9/200] [Batch 758/938] [D loss: 0.268497] [G loss: 1.741752]\n",
      "[Epoch 10/200] [Batch 220/938] [D loss: 0.370330] [G loss: 1.762713]\n",
      "[Epoch 10/200] [Batch 620/938] [D loss: 0.394382] [G loss: 1.791158]\n",
      "[Epoch 11/200] [Batch 82/938] [D loss: 0.265400] [G loss: 2.208085]\n",
      "[Epoch 11/200] [Batch 482/938] [D loss: 0.310642] [G loss: 2.354127]\n",
      "[Epoch 11/200] [Batch 882/938] [D loss: 0.369578] [G loss: 1.507875]\n",
      "[Epoch 12/200] [Batch 344/938] [D loss: 0.403450] [G loss: 1.694796]\n",
      "[Epoch 12/200] [Batch 744/938] [D loss: 0.249673] [G loss: 1.832714]\n",
      "[Epoch 13/200] [Batch 206/938] [D loss: 0.263412] [G loss: 1.674768]\n",
      "[Epoch 13/200] [Batch 606/938] [D loss: 0.354136] [G loss: 1.796344]\n",
      "[Epoch 14/200] [Batch 68/938] [D loss: 0.268232] [G loss: 1.899011]\n",
      "[Epoch 14/200] [Batch 468/938] [D loss: 0.322261] [G loss: 2.070394]\n",
      "[Epoch 14/200] [Batch 868/938] [D loss: 0.543579] [G loss: 0.618476]\n",
      "[Epoch 15/200] [Batch 330/938] [D loss: 0.320065] [G loss: 1.546836]\n",
      "[Epoch 15/200] [Batch 730/938] [D loss: 0.270493] [G loss: 2.409631]\n",
      "[Epoch 16/200] [Batch 192/938] [D loss: 0.283002] [G loss: 2.521539]\n",
      "[Epoch 16/200] [Batch 592/938] [D loss: 0.308421] [G loss: 1.614684]\n",
      "[Epoch 17/200] [Batch 54/938] [D loss: 0.378159] [G loss: 1.454362]\n",
      "[Epoch 17/200] [Batch 454/938] [D loss: 0.310802] [G loss: 1.254430]\n",
      "[Epoch 17/200] [Batch 854/938] [D loss: 0.416498] [G loss: 1.201215]\n",
      "[Epoch 18/200] [Batch 316/938] [D loss: 0.312367] [G loss: 1.494447]\n",
      "[Epoch 18/200] [Batch 716/938] [D loss: 0.468623] [G loss: 1.428954]\n",
      "[Epoch 19/200] [Batch 178/938] [D loss: 0.387319] [G loss: 1.189369]\n",
      "[Epoch 19/200] [Batch 578/938] [D loss: 0.313545] [G loss: 1.419100]\n",
      "[Epoch 20/200] [Batch 40/938] [D loss: 0.448641] [G loss: 2.308599]\n",
      "[Epoch 20/200] [Batch 440/938] [D loss: 0.489539] [G loss: 2.336977]\n",
      "[Epoch 20/200] [Batch 840/938] [D loss: 0.493563] [G loss: 1.315465]\n",
      "[Epoch 21/200] [Batch 302/938] [D loss: 0.390114] [G loss: 1.005739]\n",
      "[Epoch 21/200] [Batch 702/938] [D loss: 0.318349] [G loss: 1.816641]\n",
      "[Epoch 22/200] [Batch 164/938] [D loss: 0.393036] [G loss: 1.271457]\n",
      "[Epoch 22/200] [Batch 564/938] [D loss: 0.388178] [G loss: 1.473744]\n",
      "[Epoch 23/200] [Batch 26/938] [D loss: 0.515833] [G loss: 0.831517]\n",
      "[Epoch 23/200] [Batch 426/938] [D loss: 0.519939] [G loss: 1.507543]\n",
      "[Epoch 23/200] [Batch 826/938] [D loss: 0.411379] [G loss: 1.540380]\n",
      "[Epoch 24/200] [Batch 288/938] [D loss: 0.384604] [G loss: 1.481027]\n",
      "[Epoch 24/200] [Batch 688/938] [D loss: 0.410976] [G loss: 1.438277]\n",
      "[Epoch 25/200] [Batch 150/938] [D loss: 0.341134] [G loss: 1.670004]\n",
      "[Epoch 25/200] [Batch 550/938] [D loss: 0.472605] [G loss: 0.867663]\n",
      "[Epoch 26/200] [Batch 12/938] [D loss: 0.537326] [G loss: 0.824567]\n",
      "[Epoch 26/200] [Batch 412/938] [D loss: 0.428478] [G loss: 1.045026]\n",
      "[Epoch 26/200] [Batch 812/938] [D loss: 0.444049] [G loss: 1.178430]\n",
      "[Epoch 27/200] [Batch 274/938] [D loss: 0.520136] [G loss: 1.114455]\n",
      "[Epoch 27/200] [Batch 674/938] [D loss: 0.386565] [G loss: 1.563111]\n",
      "[Epoch 28/200] [Batch 136/938] [D loss: 0.461053] [G loss: 1.229214]\n",
      "[Epoch 28/200] [Batch 536/938] [D loss: 0.485880] [G loss: 1.569730]\n",
      "[Epoch 28/200] [Batch 936/938] [D loss: 0.524573] [G loss: 1.218656]\n",
      "[Epoch 29/200] [Batch 398/938] [D loss: 0.362424] [G loss: 1.541363]\n",
      "[Epoch 29/200] [Batch 798/938] [D loss: 0.420677] [G loss: 1.473130]\n",
      "[Epoch 30/200] [Batch 260/938] [D loss: 0.451795] [G loss: 1.313615]\n",
      "[Epoch 30/200] [Batch 660/938] [D loss: 0.427692] [G loss: 1.767761]\n",
      "[Epoch 31/200] [Batch 122/938] [D loss: 0.432433] [G loss: 1.662608]\n",
      "[Epoch 31/200] [Batch 522/938] [D loss: 0.423770] [G loss: 1.545522]\n",
      "[Epoch 31/200] [Batch 922/938] [D loss: 0.394756] [G loss: 1.488294]\n",
      "[Epoch 32/200] [Batch 384/938] [D loss: 0.489614] [G loss: 1.026406]\n",
      "[Epoch 32/200] [Batch 784/938] [D loss: 0.491343] [G loss: 2.288352]\n",
      "[Epoch 33/200] [Batch 246/938] [D loss: 0.509478] [G loss: 1.485937]\n",
      "[Epoch 33/200] [Batch 646/938] [D loss: 0.449803] [G loss: 1.098052]\n",
      "[Epoch 34/200] [Batch 108/938] [D loss: 0.593354] [G loss: 1.485679]\n",
      "[Epoch 34/200] [Batch 508/938] [D loss: 0.474244] [G loss: 1.158900]\n",
      "[Epoch 34/200] [Batch 908/938] [D loss: 0.468379] [G loss: 1.675299]\n",
      "[Epoch 35/200] [Batch 370/938] [D loss: 0.528103] [G loss: 1.765760]\n",
      "[Epoch 35/200] [Batch 770/938] [D loss: 0.561214] [G loss: 0.740882]\n",
      "[Epoch 36/200] [Batch 232/938] [D loss: 0.376560] [G loss: 1.857190]\n",
      "[Epoch 36/200] [Batch 632/938] [D loss: 0.496738] [G loss: 1.460620]\n",
      "[Epoch 37/200] [Batch 94/938] [D loss: 0.414269] [G loss: 1.232108]\n",
      "[Epoch 37/200] [Batch 494/938] [D loss: 0.474032] [G loss: 0.975089]\n",
      "[Epoch 37/200] [Batch 894/938] [D loss: 0.465166] [G loss: 1.347635]\n",
      "[Epoch 38/200] [Batch 356/938] [D loss: 0.442277] [G loss: 1.660977]\n",
      "[Epoch 38/200] [Batch 756/938] [D loss: 0.504511] [G loss: 1.191776]\n",
      "[Epoch 39/200] [Batch 218/938] [D loss: 0.382219] [G loss: 1.480244]\n",
      "[Epoch 39/200] [Batch 618/938] [D loss: 0.548311] [G loss: 1.908851]\n",
      "[Epoch 40/200] [Batch 80/938] [D loss: 0.457839] [G loss: 1.225648]\n",
      "[Epoch 40/200] [Batch 480/938] [D loss: 0.511223] [G loss: 0.895010]\n",
      "[Epoch 40/200] [Batch 880/938] [D loss: 0.521550] [G loss: 1.056636]\n",
      "[Epoch 41/200] [Batch 342/938] [D loss: 0.535694] [G loss: 1.281830]\n",
      "[Epoch 41/200] [Batch 742/938] [D loss: 0.560143] [G loss: 1.860269]\n",
      "[Epoch 42/200] [Batch 204/938] [D loss: 0.421034] [G loss: 1.168239]\n",
      "[Epoch 42/200] [Batch 604/938] [D loss: 0.372635] [G loss: 1.610922]\n",
      "[Epoch 43/200] [Batch 66/938] [D loss: 0.509725] [G loss: 1.020281]\n",
      "[Epoch 43/200] [Batch 466/938] [D loss: 0.420023] [G loss: 1.232243]\n",
      "[Epoch 43/200] [Batch 866/938] [D loss: 0.505186] [G loss: 1.354963]\n",
      "[Epoch 44/200] [Batch 328/938] [D loss: 0.397322] [G loss: 1.410674]\n",
      "[Epoch 44/200] [Batch 728/938] [D loss: 0.386993] [G loss: 1.386602]\n",
      "[Epoch 45/200] [Batch 190/938] [D loss: 0.450831] [G loss: 1.255596]\n",
      "[Epoch 45/200] [Batch 590/938] [D loss: 0.372203] [G loss: 1.202554]\n",
      "[Epoch 46/200] [Batch 52/938] [D loss: 0.504480] [G loss: 0.934295]\n",
      "[Epoch 46/200] [Batch 452/938] [D loss: 0.419570] [G loss: 1.778698]\n",
      "[Epoch 46/200] [Batch 852/938] [D loss: 0.422906] [G loss: 1.536069]\n",
      "[Epoch 47/200] [Batch 314/938] [D loss: 0.482796] [G loss: 0.901284]\n",
      "[Epoch 47/200] [Batch 714/938] [D loss: 0.498434] [G loss: 1.911978]\n",
      "[Epoch 48/200] [Batch 176/938] [D loss: 0.416587] [G loss: 1.812472]\n",
      "[Epoch 48/200] [Batch 576/938] [D loss: 0.483446] [G loss: 1.348547]\n",
      "[Epoch 49/200] [Batch 38/938] [D loss: 0.443220] [G loss: 1.474041]\n",
      "[Epoch 49/200] [Batch 438/938] [D loss: 0.511579] [G loss: 0.700674]\n",
      "[Epoch 49/200] [Batch 838/938] [D loss: 0.468790] [G loss: 1.506903]\n",
      "[Epoch 50/200] [Batch 300/938] [D loss: 0.524609] [G loss: 1.371176]\n",
      "[Epoch 50/200] [Batch 700/938] [D loss: 0.389724] [G loss: 1.057449]\n",
      "[Epoch 51/200] [Batch 162/938] [D loss: 0.425811] [G loss: 1.248994]\n",
      "[Epoch 51/200] [Batch 562/938] [D loss: 0.427837] [G loss: 1.568118]\n",
      "[Epoch 52/200] [Batch 24/938] [D loss: 0.503518] [G loss: 0.949975]\n",
      "[Epoch 52/200] [Batch 424/938] [D loss: 0.520019] [G loss: 1.775985]\n",
      "[Epoch 52/200] [Batch 824/938] [D loss: 0.402066] [G loss: 1.382131]\n",
      "[Epoch 53/200] [Batch 286/938] [D loss: 0.474041] [G loss: 1.341567]\n",
      "[Epoch 53/200] [Batch 686/938] [D loss: 0.427751] [G loss: 1.232344]\n",
      "[Epoch 54/200] [Batch 148/938] [D loss: 0.397424] [G loss: 1.341710]\n",
      "[Epoch 54/200] [Batch 548/938] [D loss: 0.406682] [G loss: 1.361021]\n",
      "[Epoch 55/200] [Batch 10/938] [D loss: 0.386835] [G loss: 1.482378]\n",
      "[Epoch 55/200] [Batch 410/938] [D loss: 0.443944] [G loss: 1.352189]\n",
      "[Epoch 55/200] [Batch 810/938] [D loss: 0.386898] [G loss: 1.507890]\n",
      "[Epoch 56/200] [Batch 272/938] [D loss: 0.508651] [G loss: 1.452193]\n",
      "[Epoch 56/200] [Batch 672/938] [D loss: 0.499531] [G loss: 1.244572]\n",
      "[Epoch 57/200] [Batch 134/938] [D loss: 0.438959] [G loss: 1.760403]\n",
      "[Epoch 57/200] [Batch 534/938] [D loss: 0.456479] [G loss: 1.452143]\n",
      "[Epoch 57/200] [Batch 934/938] [D loss: 0.481243] [G loss: 1.108937]\n",
      "[Epoch 58/200] [Batch 396/938] [D loss: 0.483377] [G loss: 1.231325]\n",
      "[Epoch 58/200] [Batch 796/938] [D loss: 0.466811] [G loss: 1.398236]\n",
      "[Epoch 59/200] [Batch 258/938] [D loss: 0.453951] [G loss: 1.259609]\n",
      "[Epoch 59/200] [Batch 658/938] [D loss: 0.457430] [G loss: 1.538519]\n",
      "[Epoch 60/200] [Batch 120/938] [D loss: 0.476288] [G loss: 1.887751]\n",
      "[Epoch 60/200] [Batch 520/938] [D loss: 0.456663] [G loss: 1.290456]\n",
      "[Epoch 60/200] [Batch 920/938] [D loss: 0.497248] [G loss: 1.215973]\n",
      "[Epoch 61/200] [Batch 382/938] [D loss: 0.464119] [G loss: 1.425750]\n",
      "[Epoch 61/200] [Batch 782/938] [D loss: 0.685729] [G loss: 1.777226]\n",
      "[Epoch 62/200] [Batch 244/938] [D loss: 0.378830] [G loss: 1.387537]\n",
      "[Epoch 62/200] [Batch 644/938] [D loss: 0.359542] [G loss: 1.483772]\n",
      "[Epoch 63/200] [Batch 106/938] [D loss: 0.411210] [G loss: 1.271350]\n",
      "[Epoch 63/200] [Batch 506/938] [D loss: 0.480990] [G loss: 0.802942]\n",
      "[Epoch 63/200] [Batch 906/938] [D loss: 0.487545] [G loss: 1.296228]\n",
      "[Epoch 64/200] [Batch 368/938] [D loss: 0.365379] [G loss: 1.253830]\n",
      "[Epoch 64/200] [Batch 768/938] [D loss: 0.427417] [G loss: 1.413077]\n",
      "[Epoch 65/200] [Batch 230/938] [D loss: 0.467934] [G loss: 1.256068]\n",
      "[Epoch 65/200] [Batch 630/938] [D loss: 0.404969] [G loss: 1.944429]\n",
      "[Epoch 66/200] [Batch 92/938] [D loss: 0.455299] [G loss: 1.194393]\n",
      "[Epoch 66/200] [Batch 492/938] [D loss: 0.482809] [G loss: 1.329921]\n",
      "[Epoch 66/200] [Batch 892/938] [D loss: 0.414796] [G loss: 1.384885]\n",
      "[Epoch 67/200] [Batch 354/938] [D loss: 0.514904] [G loss: 1.105222]\n",
      "[Epoch 67/200] [Batch 754/938] [D loss: 0.594315] [G loss: 0.979592]\n",
      "[Epoch 68/200] [Batch 216/938] [D loss: 0.447312] [G loss: 1.608306]\n",
      "[Epoch 68/200] [Batch 616/938] [D loss: 0.428373] [G loss: 1.531325]\n",
      "[Epoch 69/200] [Batch 78/938] [D loss: 0.499465] [G loss: 1.416517]\n",
      "[Epoch 69/200] [Batch 478/938] [D loss: 0.578180] [G loss: 1.708061]\n",
      "[Epoch 69/200] [Batch 878/938] [D loss: 0.449877] [G loss: 1.180400]\n",
      "[Epoch 70/200] [Batch 340/938] [D loss: 0.546900] [G loss: 0.889762]\n",
      "[Epoch 70/200] [Batch 740/938] [D loss: 0.419985] [G loss: 1.093970]\n",
      "[Epoch 71/200] [Batch 202/938] [D loss: 0.439228] [G loss: 1.433410]\n",
      "[Epoch 71/200] [Batch 602/938] [D loss: 0.434059] [G loss: 1.074676]\n",
      "[Epoch 72/200] [Batch 64/938] [D loss: 0.428570] [G loss: 1.528682]\n",
      "[Epoch 72/200] [Batch 464/938] [D loss: 0.465040] [G loss: 1.103924]\n",
      "[Epoch 72/200] [Batch 864/938] [D loss: 0.466767] [G loss: 1.543817]\n",
      "[Epoch 73/200] [Batch 326/938] [D loss: 0.526825] [G loss: 1.330847]\n",
      "[Epoch 73/200] [Batch 726/938] [D loss: 0.486645] [G loss: 1.324468]\n",
      "[Epoch 74/200] [Batch 188/938] [D loss: 0.521171] [G loss: 0.990968]\n",
      "[Epoch 74/200] [Batch 588/938] [D loss: 0.455662] [G loss: 1.076368]\n",
      "[Epoch 75/200] [Batch 50/938] [D loss: 0.423674] [G loss: 1.350641]\n",
      "[Epoch 75/200] [Batch 450/938] [D loss: 0.400726] [G loss: 1.721977]\n",
      "[Epoch 75/200] [Batch 850/938] [D loss: 0.464776] [G loss: 1.263600]\n",
      "[Epoch 76/200] [Batch 312/938] [D loss: 0.501061] [G loss: 1.829017]\n",
      "[Epoch 76/200] [Batch 712/938] [D loss: 0.506941] [G loss: 1.455279]\n",
      "[Epoch 77/200] [Batch 174/938] [D loss: 0.443609] [G loss: 1.812297]\n",
      "[Epoch 77/200] [Batch 574/938] [D loss: 0.421580] [G loss: 1.009126]\n",
      "[Epoch 78/200] [Batch 36/938] [D loss: 0.510256] [G loss: 1.247893]\n",
      "[Epoch 78/200] [Batch 436/938] [D loss: 0.374646] [G loss: 1.237952]\n",
      "[Epoch 78/200] [Batch 836/938] [D loss: 0.515041] [G loss: 1.177231]\n",
      "[Epoch 79/200] [Batch 298/938] [D loss: 0.386274] [G loss: 1.558292]\n",
      "[Epoch 79/200] [Batch 698/938] [D loss: 0.490183] [G loss: 1.139818]\n",
      "[Epoch 80/200] [Batch 160/938] [D loss: 0.385022] [G loss: 1.536768]\n",
      "[Epoch 80/200] [Batch 560/938] [D loss: 0.510093] [G loss: 2.012411]\n",
      "[Epoch 81/200] [Batch 22/938] [D loss: 0.481889] [G loss: 1.662569]\n",
      "[Epoch 81/200] [Batch 422/938] [D loss: 0.330507] [G loss: 1.623285]\n",
      "[Epoch 81/200] [Batch 822/938] [D loss: 0.558393] [G loss: 1.664155]\n",
      "[Epoch 82/200] [Batch 284/938] [D loss: 0.414061] [G loss: 1.409400]\n",
      "[Epoch 82/200] [Batch 684/938] [D loss: 0.455799] [G loss: 1.813935]\n",
      "[Epoch 83/200] [Batch 146/938] [D loss: 0.455422] [G loss: 1.337312]\n",
      "[Epoch 83/200] [Batch 546/938] [D loss: 0.425198] [G loss: 1.392181]\n",
      "[Epoch 84/200] [Batch 8/938] [D loss: 0.415918] [G loss: 1.234860]\n",
      "[Epoch 84/200] [Batch 408/938] [D loss: 0.467316] [G loss: 1.591845]\n",
      "[Epoch 84/200] [Batch 808/938] [D loss: 0.421712] [G loss: 1.009380]\n",
      "[Epoch 85/200] [Batch 270/938] [D loss: 0.470611] [G loss: 1.516102]\n",
      "[Epoch 85/200] [Batch 670/938] [D loss: 0.463981] [G loss: 1.268504]\n",
      "[Epoch 86/200] [Batch 132/938] [D loss: 0.354991] [G loss: 1.442975]\n",
      "[Epoch 86/200] [Batch 532/938] [D loss: 0.443587] [G loss: 1.547628]\n",
      "[Epoch 86/200] [Batch 932/938] [D loss: 0.565451] [G loss: 1.822409]\n",
      "[Epoch 87/200] [Batch 394/938] [D loss: 0.453526] [G loss: 0.995945]\n",
      "[Epoch 87/200] [Batch 794/938] [D loss: 0.440179] [G loss: 1.272997]\n",
      "[Epoch 88/200] [Batch 256/938] [D loss: 0.357080] [G loss: 1.666139]\n",
      "[Epoch 88/200] [Batch 656/938] [D loss: 0.462347] [G loss: 1.301679]\n",
      "[Epoch 89/200] [Batch 118/938] [D loss: 0.415461] [G loss: 1.445485]\n",
      "[Epoch 89/200] [Batch 518/938] [D loss: 0.424709] [G loss: 1.642892]\n",
      "[Epoch 89/200] [Batch 918/938] [D loss: 0.447233] [G loss: 1.732966]\n",
      "[Epoch 90/200] [Batch 380/938] [D loss: 0.535307] [G loss: 1.976392]\n",
      "[Epoch 90/200] [Batch 780/938] [D loss: 0.408763] [G loss: 1.253620]\n",
      "[Epoch 91/200] [Batch 242/938] [D loss: 0.505283] [G loss: 1.452763]\n",
      "[Epoch 91/200] [Batch 642/938] [D loss: 0.386474] [G loss: 1.531119]\n",
      "[Epoch 92/200] [Batch 104/938] [D loss: 0.411880] [G loss: 1.233574]\n",
      "[Epoch 92/200] [Batch 504/938] [D loss: 0.460617] [G loss: 1.165789]\n",
      "[Epoch 92/200] [Batch 904/938] [D loss: 0.530698] [G loss: 1.615279]\n",
      "[Epoch 93/200] [Batch 366/938] [D loss: 0.455920] [G loss: 1.271836]\n",
      "[Epoch 93/200] [Batch 766/938] [D loss: 0.498293] [G loss: 1.796488]\n",
      "[Epoch 94/200] [Batch 228/938] [D loss: 0.447733] [G loss: 1.485448]\n",
      "[Epoch 94/200] [Batch 628/938] [D loss: 0.377239] [G loss: 1.337091]\n",
      "[Epoch 95/200] [Batch 90/938] [D loss: 0.402334] [G loss: 1.579391]\n",
      "[Epoch 95/200] [Batch 490/938] [D loss: 0.435523] [G loss: 1.202916]\n",
      "[Epoch 95/200] [Batch 890/938] [D loss: 0.416254] [G loss: 1.614821]\n",
      "[Epoch 96/200] [Batch 352/938] [D loss: 0.409621] [G loss: 1.614394]\n",
      "[Epoch 96/200] [Batch 752/938] [D loss: 0.415672] [G loss: 2.030583]\n",
      "[Epoch 97/200] [Batch 214/938] [D loss: 0.420862] [G loss: 1.396876]\n",
      "[Epoch 97/200] [Batch 614/938] [D loss: 0.425797] [G loss: 1.528292]\n",
      "[Epoch 98/200] [Batch 76/938] [D loss: 0.413454] [G loss: 1.612448]\n",
      "[Epoch 98/200] [Batch 476/938] [D loss: 0.514142] [G loss: 0.825484]\n",
      "[Epoch 98/200] [Batch 876/938] [D loss: 0.433041] [G loss: 1.605505]\n",
      "[Epoch 99/200] [Batch 338/938] [D loss: 0.592093] [G loss: 0.852695]\n",
      "[Epoch 99/200] [Batch 738/938] [D loss: 0.540217] [G loss: 1.528866]\n",
      "[Epoch 100/200] [Batch 200/938] [D loss: 0.419435] [G loss: 1.333464]\n",
      "[Epoch 100/200] [Batch 600/938] [D loss: 0.396047] [G loss: 1.825589]\n",
      "[Epoch 101/200] [Batch 62/938] [D loss: 0.453692] [G loss: 1.218165]\n",
      "[Epoch 101/200] [Batch 462/938] [D loss: 0.455353] [G loss: 1.128641]\n",
      "[Epoch 101/200] [Batch 862/938] [D loss: 0.394443] [G loss: 1.645101]\n",
      "[Epoch 102/200] [Batch 324/938] [D loss: 0.383251] [G loss: 1.982962]\n",
      "[Epoch 102/200] [Batch 724/938] [D loss: 0.438718] [G loss: 1.497635]\n",
      "[Epoch 103/200] [Batch 186/938] [D loss: 0.488451] [G loss: 1.437665]\n",
      "[Epoch 103/200] [Batch 586/938] [D loss: 0.527563] [G loss: 1.048315]\n",
      "[Epoch 104/200] [Batch 48/938] [D loss: 0.481259] [G loss: 1.151539]\n",
      "[Epoch 104/200] [Batch 448/938] [D loss: 0.466738] [G loss: 1.427854]\n",
      "[Epoch 104/200] [Batch 848/938] [D loss: 0.449932] [G loss: 1.145900]\n",
      "[Epoch 105/200] [Batch 310/938] [D loss: 0.458039] [G loss: 1.951744]\n",
      "[Epoch 105/200] [Batch 710/938] [D loss: 0.469553] [G loss: 1.419258]\n",
      "[Epoch 106/200] [Batch 172/938] [D loss: 0.305307] [G loss: 1.715092]\n",
      "[Epoch 106/200] [Batch 572/938] [D loss: 0.508869] [G loss: 1.547194]\n",
      "[Epoch 107/200] [Batch 34/938] [D loss: 0.577503] [G loss: 1.544587]\n",
      "[Epoch 107/200] [Batch 434/938] [D loss: 0.493246] [G loss: 1.599218]\n",
      "[Epoch 107/200] [Batch 834/938] [D loss: 0.390317] [G loss: 1.138721]\n",
      "[Epoch 108/200] [Batch 296/938] [D loss: 0.391718] [G loss: 1.342931]\n",
      "[Epoch 108/200] [Batch 696/938] [D loss: 0.443005] [G loss: 1.716968]\n",
      "[Epoch 109/200] [Batch 158/938] [D loss: 0.488462] [G loss: 1.410655]\n",
      "[Epoch 109/200] [Batch 558/938] [D loss: 0.329726] [G loss: 1.768311]\n",
      "[Epoch 110/200] [Batch 20/938] [D loss: 0.506174] [G loss: 2.169654]\n",
      "[Epoch 110/200] [Batch 420/938] [D loss: 0.484056] [G loss: 1.251078]\n",
      "[Epoch 110/200] [Batch 820/938] [D loss: 0.417157] [G loss: 1.542129]\n",
      "[Epoch 111/200] [Batch 282/938] [D loss: 0.434021] [G loss: 1.599026]\n",
      "[Epoch 111/200] [Batch 682/938] [D loss: 0.431706] [G loss: 1.423700]\n",
      "[Epoch 112/200] [Batch 144/938] [D loss: 0.449693] [G loss: 1.438351]\n",
      "[Epoch 112/200] [Batch 544/938] [D loss: 0.478495] [G loss: 1.154958]\n",
      "[Epoch 113/200] [Batch 6/938] [D loss: 0.381922] [G loss: 1.458610]\n",
      "[Epoch 113/200] [Batch 406/938] [D loss: 0.486620] [G loss: 1.526451]\n",
      "[Epoch 113/200] [Batch 806/938] [D loss: 0.393245] [G loss: 1.535457]\n",
      "[Epoch 114/200] [Batch 268/938] [D loss: 0.522299] [G loss: 1.254553]\n",
      "[Epoch 114/200] [Batch 668/938] [D loss: 0.426449] [G loss: 1.261581]\n",
      "[Epoch 115/200] [Batch 130/938] [D loss: 0.462391] [G loss: 1.513343]\n",
      "[Epoch 115/200] [Batch 530/938] [D loss: 0.432576] [G loss: 1.424618]\n",
      "[Epoch 115/200] [Batch 930/938] [D loss: 0.535424] [G loss: 1.715420]\n",
      "[Epoch 116/200] [Batch 392/938] [D loss: 0.467639] [G loss: 1.558244]\n",
      "[Epoch 116/200] [Batch 792/938] [D loss: 0.415915] [G loss: 1.161142]\n",
      "[Epoch 117/200] [Batch 254/938] [D loss: 0.412555] [G loss: 1.703601]\n",
      "[Epoch 117/200] [Batch 654/938] [D loss: 0.478321] [G loss: 1.501974]\n",
      "[Epoch 118/200] [Batch 116/938] [D loss: 0.481217] [G loss: 1.322124]\n",
      "[Epoch 118/200] [Batch 516/938] [D loss: 0.469543] [G loss: 1.354535]\n",
      "[Epoch 118/200] [Batch 916/938] [D loss: 0.450497] [G loss: 1.330627]\n",
      "[Epoch 119/200] [Batch 378/938] [D loss: 0.354714] [G loss: 1.436782]\n",
      "[Epoch 119/200] [Batch 778/938] [D loss: 0.447602] [G loss: 1.223622]\n",
      "[Epoch 120/200] [Batch 240/938] [D loss: 0.496257] [G loss: 1.129907]\n",
      "[Epoch 120/200] [Batch 640/938] [D loss: 0.489899] [G loss: 1.820981]\n",
      "[Epoch 121/200] [Batch 102/938] [D loss: 0.407380] [G loss: 1.297289]\n",
      "[Epoch 121/200] [Batch 502/938] [D loss: 0.497252] [G loss: 1.549350]\n",
      "[Epoch 121/200] [Batch 902/938] [D loss: 0.434011] [G loss: 1.444918]\n",
      "[Epoch 122/200] [Batch 364/938] [D loss: 0.450393] [G loss: 1.199955]\n",
      "[Epoch 122/200] [Batch 764/938] [D loss: 0.426387] [G loss: 1.541097]\n",
      "[Epoch 123/200] [Batch 226/938] [D loss: 0.442132] [G loss: 1.193969]\n",
      "[Epoch 123/200] [Batch 626/938] [D loss: 0.461010] [G loss: 1.918823]\n",
      "[Epoch 124/200] [Batch 88/938] [D loss: 0.448447] [G loss: 1.566134]\n",
      "[Epoch 124/200] [Batch 488/938] [D loss: 0.538710] [G loss: 0.980793]\n",
      "[Epoch 124/200] [Batch 888/938] [D loss: 0.454452] [G loss: 1.138154]\n",
      "[Epoch 125/200] [Batch 350/938] [D loss: 0.382423] [G loss: 1.321482]\n",
      "[Epoch 125/200] [Batch 750/938] [D loss: 0.515436] [G loss: 1.848637]\n",
      "[Epoch 126/200] [Batch 212/938] [D loss: 0.438369] [G loss: 0.991757]\n",
      "[Epoch 126/200] [Batch 612/938] [D loss: 0.426571] [G loss: 1.572492]\n",
      "[Epoch 127/200] [Batch 74/938] [D loss: 0.437174] [G loss: 1.210066]\n",
      "[Epoch 127/200] [Batch 474/938] [D loss: 0.416178] [G loss: 1.461640]\n",
      "[Epoch 127/200] [Batch 874/938] [D loss: 0.533929] [G loss: 1.047552]\n",
      "[Epoch 128/200] [Batch 336/938] [D loss: 0.546717] [G loss: 1.628692]\n",
      "[Epoch 128/200] [Batch 736/938] [D loss: 0.444911] [G loss: 1.264167]\n",
      "[Epoch 129/200] [Batch 198/938] [D loss: 0.467426] [G loss: 1.345461]\n",
      "[Epoch 129/200] [Batch 598/938] [D loss: 0.617337] [G loss: 1.472305]\n",
      "[Epoch 130/200] [Batch 60/938] [D loss: 0.381164] [G loss: 1.630897]\n",
      "[Epoch 130/200] [Batch 460/938] [D loss: 0.483317] [G loss: 1.098127]\n",
      "[Epoch 130/200] [Batch 860/938] [D loss: 0.454270] [G loss: 1.658613]\n",
      "[Epoch 131/200] [Batch 322/938] [D loss: 0.460737] [G loss: 1.115320]\n",
      "[Epoch 131/200] [Batch 722/938] [D loss: 0.471079] [G loss: 1.511637]\n",
      "[Epoch 132/200] [Batch 184/938] [D loss: 0.485147] [G loss: 1.043557]\n",
      "[Epoch 132/200] [Batch 584/938] [D loss: 0.508087] [G loss: 1.741638]\n",
      "[Epoch 133/200] [Batch 46/938] [D loss: 0.480417] [G loss: 1.502713]\n",
      "[Epoch 133/200] [Batch 446/938] [D loss: 0.415259] [G loss: 1.680472]\n",
      "[Epoch 133/200] [Batch 846/938] [D loss: 0.475513] [G loss: 1.157902]\n",
      "[Epoch 134/200] [Batch 308/938] [D loss: 0.540844] [G loss: 0.902745]\n",
      "[Epoch 134/200] [Batch 708/938] [D loss: 0.507567] [G loss: 1.233732]\n",
      "[Epoch 135/200] [Batch 170/938] [D loss: 0.566142] [G loss: 1.403077]\n",
      "[Epoch 135/200] [Batch 570/938] [D loss: 0.400051] [G loss: 1.387576]\n",
      "[Epoch 136/200] [Batch 32/938] [D loss: 0.468244] [G loss: 1.000475]\n",
      "[Epoch 136/200] [Batch 432/938] [D loss: 0.380449] [G loss: 1.228534]\n",
      "[Epoch 136/200] [Batch 832/938] [D loss: 0.472992] [G loss: 1.285225]\n",
      "[Epoch 137/200] [Batch 294/938] [D loss: 0.499755] [G loss: 1.220240]\n",
      "[Epoch 137/200] [Batch 694/938] [D loss: 0.491370] [G loss: 1.279703]\n",
      "[Epoch 138/200] [Batch 156/938] [D loss: 0.393983] [G loss: 1.503104]\n",
      "[Epoch 138/200] [Batch 556/938] [D loss: 0.465798] [G loss: 1.407669]\n",
      "[Epoch 139/200] [Batch 18/938] [D loss: 0.497643] [G loss: 1.192099]\n",
      "[Epoch 139/200] [Batch 418/938] [D loss: 0.472735] [G loss: 1.410782]\n",
      "[Epoch 139/200] [Batch 818/938] [D loss: 0.489279] [G loss: 1.656486]\n",
      "[Epoch 140/200] [Batch 280/938] [D loss: 0.545380] [G loss: 1.065468]\n",
      "[Epoch 140/200] [Batch 680/938] [D loss: 0.475778] [G loss: 1.550734]\n",
      "[Epoch 141/200] [Batch 142/938] [D loss: 0.557729] [G loss: 1.498107]\n",
      "[Epoch 141/200] [Batch 542/938] [D loss: 0.510140] [G loss: 1.737008]\n",
      "[Epoch 142/200] [Batch 4/938] [D loss: 0.478971] [G loss: 1.234716]\n",
      "[Epoch 142/200] [Batch 404/938] [D loss: 0.535824] [G loss: 1.225515]\n",
      "[Epoch 142/200] [Batch 804/938] [D loss: 0.483944] [G loss: 1.369460]\n",
      "[Epoch 143/200] [Batch 266/938] [D loss: 0.437268] [G loss: 1.298219]\n",
      "[Epoch 143/200] [Batch 666/938] [D loss: 0.487456] [G loss: 1.169326]\n",
      "[Epoch 144/200] [Batch 128/938] [D loss: 0.395740] [G loss: 1.570184]\n",
      "[Epoch 144/200] [Batch 528/938] [D loss: 0.408886] [G loss: 1.312631]\n",
      "[Epoch 144/200] [Batch 928/938] [D loss: 0.422039] [G loss: 1.467607]\n",
      "[Epoch 145/200] [Batch 390/938] [D loss: 0.422768] [G loss: 1.088047]\n",
      "[Epoch 145/200] [Batch 790/938] [D loss: 0.552080] [G loss: 1.143204]\n",
      "[Epoch 146/200] [Batch 252/938] [D loss: 0.522097] [G loss: 1.574110]\n",
      "[Epoch 146/200] [Batch 652/938] [D loss: 0.426156] [G loss: 1.551022]\n",
      "[Epoch 147/200] [Batch 114/938] [D loss: 0.472467] [G loss: 1.391292]\n",
      "[Epoch 147/200] [Batch 514/938] [D loss: 0.465151] [G loss: 1.432815]\n",
      "[Epoch 147/200] [Batch 914/938] [D loss: 0.541243] [G loss: 1.403877]\n",
      "[Epoch 148/200] [Batch 376/938] [D loss: 0.464266] [G loss: 1.468539]\n",
      "[Epoch 148/200] [Batch 776/938] [D loss: 0.483208] [G loss: 1.254342]\n",
      "[Epoch 149/200] [Batch 238/938] [D loss: 0.482723] [G loss: 1.429193]\n",
      "[Epoch 149/200] [Batch 638/938] [D loss: 0.553175] [G loss: 1.180848]\n",
      "[Epoch 150/200] [Batch 100/938] [D loss: 0.507544] [G loss: 1.225239]\n",
      "[Epoch 150/200] [Batch 500/938] [D loss: 0.503121] [G loss: 0.927812]\n",
      "[Epoch 150/200] [Batch 900/938] [D loss: 0.514006] [G loss: 1.487926]\n",
      "[Epoch 151/200] [Batch 362/938] [D loss: 0.541488] [G loss: 1.120934]\n",
      "[Epoch 151/200] [Batch 762/938] [D loss: 0.408452] [G loss: 1.426070]\n",
      "[Epoch 152/200] [Batch 224/938] [D loss: 0.508010] [G loss: 0.865298]\n",
      "[Epoch 152/200] [Batch 624/938] [D loss: 0.461137] [G loss: 1.362363]\n",
      "[Epoch 153/200] [Batch 86/938] [D loss: 0.451720] [G loss: 1.289940]\n",
      "[Epoch 153/200] [Batch 486/938] [D loss: 0.433628] [G loss: 1.493519]\n",
      "[Epoch 153/200] [Batch 886/938] [D loss: 0.437286] [G loss: 1.287820]\n",
      "[Epoch 154/200] [Batch 348/938] [D loss: 0.433216] [G loss: 1.399416]\n",
      "[Epoch 154/200] [Batch 748/938] [D loss: 0.369322] [G loss: 2.037946]\n",
      "[Epoch 155/200] [Batch 210/938] [D loss: 0.392159] [G loss: 1.345384]\n",
      "[Epoch 155/200] [Batch 610/938] [D loss: 0.475304] [G loss: 1.414621]\n",
      "[Epoch 156/200] [Batch 72/938] [D loss: 0.484300] [G loss: 1.761739]\n",
      "[Epoch 156/200] [Batch 472/938] [D loss: 0.428776] [G loss: 1.178145]\n",
      "[Epoch 156/200] [Batch 872/938] [D loss: 0.493814] [G loss: 1.377075]\n",
      "[Epoch 157/200] [Batch 334/938] [D loss: 0.507842] [G loss: 1.333437]\n",
      "[Epoch 157/200] [Batch 734/938] [D loss: 0.419190] [G loss: 1.388079]\n",
      "[Epoch 158/200] [Batch 196/938] [D loss: 0.449650] [G loss: 1.466358]\n",
      "[Epoch 158/200] [Batch 596/938] [D loss: 0.540626] [G loss: 1.210172]\n",
      "[Epoch 159/200] [Batch 58/938] [D loss: 0.481121] [G loss: 1.304061]\n",
      "[Epoch 159/200] [Batch 458/938] [D loss: 0.520444] [G loss: 1.499262]\n",
      "[Epoch 159/200] [Batch 858/938] [D loss: 0.575649] [G loss: 1.490657]\n",
      "[Epoch 160/200] [Batch 320/938] [D loss: 0.526479] [G loss: 1.724742]\n",
      "[Epoch 160/200] [Batch 720/938] [D loss: 0.446250] [G loss: 1.209804]\n",
      "[Epoch 161/200] [Batch 182/938] [D loss: 0.488716] [G loss: 1.235718]\n",
      "[Epoch 161/200] [Batch 582/938] [D loss: 0.406070] [G loss: 1.341120]\n",
      "[Epoch 162/200] [Batch 44/938] [D loss: 0.443588] [G loss: 1.476792]\n",
      "[Epoch 162/200] [Batch 444/938] [D loss: 0.479897] [G loss: 1.522997]\n",
      "[Epoch 162/200] [Batch 844/938] [D loss: 0.430852] [G loss: 1.265549]\n",
      "[Epoch 163/200] [Batch 306/938] [D loss: 0.440848] [G loss: 1.553633]\n",
      "[Epoch 163/200] [Batch 706/938] [D loss: 0.418377] [G loss: 1.208925]\n",
      "[Epoch 164/200] [Batch 168/938] [D loss: 0.506828] [G loss: 1.327276]\n",
      "[Epoch 164/200] [Batch 568/938] [D loss: 0.431084] [G loss: 1.540802]\n",
      "[Epoch 165/200] [Batch 30/938] [D loss: 0.433872] [G loss: 1.234632]\n",
      "[Epoch 165/200] [Batch 430/938] [D loss: 0.508077] [G loss: 1.344534]\n",
      "[Epoch 165/200] [Batch 830/938] [D loss: 0.408225] [G loss: 1.393731]\n",
      "[Epoch 166/200] [Batch 292/938] [D loss: 0.395589] [G loss: 1.460539]\n",
      "[Epoch 166/200] [Batch 692/938] [D loss: 0.468477] [G loss: 1.792771]\n",
      "[Epoch 167/200] [Batch 154/938] [D loss: 0.395085] [G loss: 1.427284]\n",
      "[Epoch 167/200] [Batch 554/938] [D loss: 0.539563] [G loss: 1.441429]\n",
      "[Epoch 168/200] [Batch 16/938] [D loss: 0.503807] [G loss: 1.405970]\n",
      "[Epoch 168/200] [Batch 416/938] [D loss: 0.369594] [G loss: 1.343018]\n",
      "[Epoch 168/200] [Batch 816/938] [D loss: 0.540737] [G loss: 1.154909]\n",
      "[Epoch 169/200] [Batch 278/938] [D loss: 0.427378] [G loss: 1.669279]\n",
      "[Epoch 169/200] [Batch 678/938] [D loss: 0.494888] [G loss: 1.446006]\n",
      "[Epoch 170/200] [Batch 140/938] [D loss: 0.416771] [G loss: 1.322716]\n",
      "[Epoch 170/200] [Batch 540/938] [D loss: 0.492666] [G loss: 1.536225]\n",
      "[Epoch 171/200] [Batch 2/938] [D loss: 0.480911] [G loss: 1.521412]\n",
      "[Epoch 171/200] [Batch 402/938] [D loss: 0.466919] [G loss: 1.224195]\n",
      "[Epoch 171/200] [Batch 802/938] [D loss: 0.412797] [G loss: 1.475355]\n",
      "[Epoch 172/200] [Batch 264/938] [D loss: 0.546338] [G loss: 1.380309]\n",
      "[Epoch 172/200] [Batch 664/938] [D loss: 0.434666] [G loss: 1.389499]\n",
      "[Epoch 173/200] [Batch 126/938] [D loss: 0.425164] [G loss: 1.319557]\n",
      "[Epoch 173/200] [Batch 526/938] [D loss: 0.466767] [G loss: 1.490160]\n",
      "[Epoch 173/200] [Batch 926/938] [D loss: 0.504465] [G loss: 1.564236]\n",
      "[Epoch 174/200] [Batch 388/938] [D loss: 0.422732] [G loss: 1.515147]\n",
      "[Epoch 174/200] [Batch 788/938] [D loss: 0.511961] [G loss: 1.672514]\n",
      "[Epoch 175/200] [Batch 250/938] [D loss: 0.493706] [G loss: 1.016306]\n",
      "[Epoch 175/200] [Batch 650/938] [D loss: 0.468506] [G loss: 1.438685]\n",
      "[Epoch 176/200] [Batch 112/938] [D loss: 0.418633] [G loss: 1.513844]\n",
      "[Epoch 176/200] [Batch 512/938] [D loss: 0.417801] [G loss: 1.492914]\n",
      "[Epoch 176/200] [Batch 912/938] [D loss: 0.500866] [G loss: 1.318925]\n",
      "[Epoch 177/200] [Batch 374/938] [D loss: 0.478198] [G loss: 1.511648]\n",
      "[Epoch 177/200] [Batch 774/938] [D loss: 0.462501] [G loss: 1.666007]\n",
      "[Epoch 178/200] [Batch 236/938] [D loss: 0.512169] [G loss: 1.385475]\n",
      "[Epoch 178/200] [Batch 636/938] [D loss: 0.516160] [G loss: 1.460361]\n",
      "[Epoch 179/200] [Batch 98/938] [D loss: 0.368187] [G loss: 1.457730]\n",
      "[Epoch 179/200] [Batch 498/938] [D loss: 0.553036] [G loss: 1.470605]\n",
      "[Epoch 179/200] [Batch 898/938] [D loss: 0.454830] [G loss: 1.323158]\n",
      "[Epoch 180/200] [Batch 360/938] [D loss: 0.374105] [G loss: 1.506218]\n",
      "[Epoch 180/200] [Batch 760/938] [D loss: 0.401567] [G loss: 1.680789]\n",
      "[Epoch 181/200] [Batch 222/938] [D loss: 0.497263] [G loss: 0.996598]\n",
      "[Epoch 181/200] [Batch 622/938] [D loss: 0.422179] [G loss: 1.390783]\n",
      "[Epoch 182/200] [Batch 84/938] [D loss: 0.493498] [G loss: 1.132367]\n",
      "[Epoch 182/200] [Batch 484/938] [D loss: 0.406451] [G loss: 1.524219]\n",
      "[Epoch 182/200] [Batch 884/938] [D loss: 0.447167] [G loss: 1.384784]\n",
      "[Epoch 183/200] [Batch 346/938] [D loss: 0.402850] [G loss: 1.272831]\n",
      "[Epoch 183/200] [Batch 746/938] [D loss: 0.457625] [G loss: 1.345604]\n",
      "[Epoch 184/200] [Batch 208/938] [D loss: 0.485250] [G loss: 1.574166]\n",
      "[Epoch 184/200] [Batch 608/938] [D loss: 0.404492] [G loss: 1.465484]\n",
      "[Epoch 185/200] [Batch 70/938] [D loss: 0.436757] [G loss: 1.438253]\n",
      "[Epoch 185/200] [Batch 470/938] [D loss: 0.465044] [G loss: 1.320517]\n",
      "[Epoch 185/200] [Batch 870/938] [D loss: 0.524684] [G loss: 1.042489]\n",
      "[Epoch 186/200] [Batch 332/938] [D loss: 0.467192] [G loss: 1.588209]\n",
      "[Epoch 186/200] [Batch 732/938] [D loss: 0.460357] [G loss: 1.323926]\n",
      "[Epoch 187/200] [Batch 194/938] [D loss: 0.409186] [G loss: 1.837207]\n",
      "[Epoch 187/200] [Batch 594/938] [D loss: 0.356627] [G loss: 1.724088]\n",
      "[Epoch 188/200] [Batch 56/938] [D loss: 0.382477] [G loss: 1.355584]\n",
      "[Epoch 188/200] [Batch 456/938] [D loss: 0.441354] [G loss: 1.186960]\n",
      "[Epoch 188/200] [Batch 856/938] [D loss: 0.429907] [G loss: 1.262520]\n",
      "[Epoch 189/200] [Batch 318/938] [D loss: 0.500575] [G loss: 1.712623]\n",
      "[Epoch 189/200] [Batch 718/938] [D loss: 0.395779] [G loss: 1.374831]\n",
      "[Epoch 190/200] [Batch 180/938] [D loss: 0.390268] [G loss: 1.392041]\n",
      "[Epoch 190/200] [Batch 580/938] [D loss: 0.437247] [G loss: 1.446019]\n",
      "[Epoch 191/200] [Batch 42/938] [D loss: 0.502624] [G loss: 1.244075]\n",
      "[Epoch 191/200] [Batch 442/938] [D loss: 0.434045] [G loss: 1.183894]\n",
      "[Epoch 191/200] [Batch 842/938] [D loss: 0.429686] [G loss: 1.679274]\n",
      "[Epoch 192/200] [Batch 304/938] [D loss: 0.452909] [G loss: 1.217440]\n",
      "[Epoch 192/200] [Batch 704/938] [D loss: 0.413763] [G loss: 1.521735]\n",
      "[Epoch 193/200] [Batch 166/938] [D loss: 0.489190] [G loss: 1.233359]\n",
      "[Epoch 193/200] [Batch 566/938] [D loss: 0.498260] [G loss: 1.377282]\n",
      "[Epoch 194/200] [Batch 28/938] [D loss: 0.548628] [G loss: 1.570948]\n",
      "[Epoch 194/200] [Batch 428/938] [D loss: 0.471313] [G loss: 1.629943]\n",
      "[Epoch 194/200] [Batch 828/938] [D loss: 0.437095] [G loss: 1.355731]\n",
      "[Epoch 195/200] [Batch 290/938] [D loss: 0.347150] [G loss: 1.752099]\n",
      "[Epoch 195/200] [Batch 690/938] [D loss: 0.508113] [G loss: 1.439427]\n",
      "[Epoch 196/200] [Batch 152/938] [D loss: 0.427396] [G loss: 1.433111]\n",
      "[Epoch 196/200] [Batch 552/938] [D loss: 0.431443] [G loss: 1.281301]\n",
      "[Epoch 197/200] [Batch 14/938] [D loss: 0.478607] [G loss: 1.279247]\n",
      "[Epoch 197/200] [Batch 414/938] [D loss: 0.407760] [G loss: 1.410231]\n",
      "[Epoch 197/200] [Batch 814/938] [D loss: 0.479982] [G loss: 1.198825]\n",
      "[Epoch 198/200] [Batch 276/938] [D loss: 0.427040] [G loss: 1.221090]\n",
      "[Epoch 198/200] [Batch 676/938] [D loss: 0.469480] [G loss: 1.421305]\n",
      "[Epoch 199/200] [Batch 138/938] [D loss: 0.397610] [G loss: 1.322485]\n",
      "[Epoch 199/200] [Batch 538/938] [D loss: 0.487258] [G loss: 1.308314]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs,a) in enumerate(dataloader):\n",
    "        #print(a)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        #print(real_imgs.size())\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval/2 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        \n",
    "        if batches_done % sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images-fashion/%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
